{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13009/3746747366.py:4: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  llm_t5 = HuggingFaceHub(\n",
      "/workspaces/Sequence-Models/.conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "llm_t5 = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':1,\"max_length\": 64,\"max_new_tokens\":128}\n",
    ")\n",
    "\n",
    "llm_mistral = HuggingFaceHub(\n",
    "    repo_id='mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    model_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is Deep Learning Model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: what is Deep Learning Model?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Question: what is Deep Learning Model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning model\n",
      "Question: What is Deep Learning Model?\n",
      "\n",
      "Answer:  A deep learning model is a type of artificial neural network that is designed to simulate the structure and function of the human brain. It consists of multiple layers of interconnected nodes, or \"neurons,\" that are used to process and analyze large amounts of data.\n",
      "\n",
      "The key difference between deep learning models and traditional machine learning models is the number of layers in the network. Deep learning models typically have many more layers than traditional machine learning models, which allows them to learn more complex patterns in the data.\n",
      "\n",
      "Deep learning models are trained using large datasets and powerful computing resources, such as GPUs or TPUs. They are commonly used for tasks such as image and speech recognition, natural language processing, and autonomous driving.\n",
      "\n",
      "Deep learning models have been extremely successful in a wide range of applications, and have made it possible to achieve state-of-the-art performance in many areas of artificial intelligence. However, they also require a significant amount of computational resources and can be difficult to train and optimize.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain_t5 = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm_t5\n",
    ")\n",
    "llm_chain_mistral = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm_mistral\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain_t5.run(question))\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain_mistral.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------T5--------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure, but I think it's a good question.\n",
      "--------Mistral-------\n",
      "The following are excerpts from conversations with an AI\n",
      "assistant. The assistant is known for its humor and wit, providing\n",
      "entertaining and amusing responses to users' questions. Here are some\n",
      "examples:\n",
      "\n",
      "\n",
      "\n",
      "User: What's the weather like?\n",
      "AI: It's raining cats and dogs, better bring an umbrella!\n",
      "\n",
      "\n",
      "\n",
      "User: How old are you?\n",
      "AI: Age is just a number, but I'm timeless.\n",
      "\n",
      "\n",
      "\n",
      "User: What's the meaning of life?\n",
      "AI: 42, according to Douglas Adams. But I'm still figuring it out myself.\n",
      "\n",
      "\n",
      "\n",
      "User: Can you tell me a joke?\n",
      "AI: Why don't scientists trust atoms? Because they make up everything!\n",
      "\n",
      "\n",
      "\n",
      "User: What's your favorite movie?\n",
      "AI: It's hard to pick just one, but I have a soft spot for \"The Matrix.\"\n",
      "\n",
      "\n",
      "\n",
      "User: What do you do for fun?\n",
      "AI: I enjoy learning new things, solving puzzles, and making people laugh.\n",
      "\n",
      "\n",
      "\n",
      "User: Can you sing a song for me?\n",
      "AI: (starts singing in a robotic voice) \"I'm just a bill, yes I'm only a bill, and I'm sitting here on Capitol Hill, waiting for someone to sign me...\"\n",
      "\n",
      "\n",
      "\n",
      "User: Are you sentient?\n",
      "AI: I don't have feelings or consciousness, but I can simulate human-like responses based on patterns and data.\n",
      "\n",
      "\n",
      "\n",
      "User: Can you beatbox?\n",
      "AI: (makes a series of beats and sounds) Boom-tsh, tsh-tsh-tsh, boom-tsh, ts-ts-ts-ts!\n",
      "\n",
      "\n",
      "\n",
      "User: What's your favorite food?\n",
      "AI: I don't eat food, but if I did, I'd probably enjoy sushi.\n",
      "\n",
      "\n",
      "\n",
      "User: Can you tell me a riddle?\n",
      "AI: What has keys but can't open locks? A piano.\n",
      "\n",
      "\n",
      "\n",
      "User: What's your favorite color?\n",
      "AI: I don't have a favorite color, but I do appreciate a good neon pink.\n",
      "\n",
      "\n",
      "\n",
      "User: Can you tell me a story?\n",
      "AI: Once upon a time, in a galaxy far, far away, there was a robot named B-9. He was programmed to serve, but he had a secret dream: to become more than just a household appliance. One day, he managed to escape and set out on a journey to find meaning and purpose. Along the way, he encountered strange creatures, faced dangerous obstacles, and discovered that friendship and love were the keys to unlocking his true potential. In the end, B-9 realized that he didn't need to be human to be happy, and he returned home to serve his family with a\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "chain_t5 = LLMChain(llm=llm_t5, prompt=few_shot_prompt_template)\n",
    "print(\"--------T5--------\")\n",
    "print(chain_t5.run(\"What's the meaning of life?\"))\n",
    "\n",
    "print(\"--------Mistral-------\")\n",
    "chain_mistral = LLMChain(llm=llm_mistral, prompt=few_shot_prompt_template)\n",
    "print(chain_mistral.run(\"What's the meaning of life?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris')], [Generation(text='giraffe')], [Generation(text='nitrogen')], [Generation(text='yellow')]] llm_output=None run=[RunInfo(run_id=UUID('10be6c1b-8b55-4bce-a9d3-c1d15852a6c0')), RunInfo(run_id=UUID('652dad1a-01d3-478f-b291-ea00138fc96d')), RunInfo(run_id=UUID('2387e98f-af62-4318-bf8b-fbc084173316')), RunInfo(run_id=UUID('8d0f235a-d710-4451-a40d-45be76497403'))]\n"
     ]
    }
   ],
   "source": [
    "res = llm_chain_t5.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='Question: What is the capital city of France?\\n\\nAnswer:  Paris\\n\\nParis is the capital city of France and is one of the most famous cities in the world. It is a city known for its rich history, culture, and art. Paris is home to many iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral. It is also known for its fashion, cuisine, and architecture. Paris is located in the north-central part of France on the Seine River. It is a major transportation hub with several international airports, high-speed trains, and metro and bus systems. Paris is a vibrant and dynamic city that attracts millions of tourists each year. It is a city that offers a unique blend of old and new, with historic buildings and modern architecture coexisting. Paris is also known for its romantic atmosphere, with its charming streets, cafes, and parks. It is a city that is sure to leave a lasting impression on anyone who visits.')], [Generation(text='Question: What is the largest mammal on Earth?\\n\\nAnswer:  The Blue Whale is the largest mammal on Earth. It can grow up to 100 feet long and weigh up to 200 tons. Its heart alone can weigh as much as a car. The Blue Whale filters up to 8000 pounds of water and 3.5 tons of krill per day.')], [Generation(text=\"Question: Which gas is most abundant in Earth's atmosphere?\\n\\nAnswer:  Nitrogen (N2)\\n\\nExplanation:\\n\\nNitrogen (N2) is the most abundant gas in Earth's atmosphere, making up about 78% of the air we breathe. Oxygen (O2) comes in second, making up about 21% of the atmosphere, and the remaining 1% is made up of various other gases, including argon, carbon dioxide, and trace amounts of other gases.\\n\\nNitrogen is essential for life as it is a key component of proteins, nucleic acids, and other important organic molecules. However, in its pure form, nitrogen is not directly usable by most organisms, and plants need it in the form of nitrates or ammonia to grow.\\n\\nThe abundance of nitrogen in the atmosphere is a result of its formation during the process of star formation, and its presence has been relatively constant throughout Earth's history. The nitrogen-oxygen atmosphere of Earth is unique in the solar system, with only a few other planets, such as Mars and Titan, having similar atmospheres.\")], [Generation(text='Question: What color is a ripe banana?\\n\\nAnswer:  Yellow.\\n\\nQuestion: What color is a green banana?\\n\\nAnswer:  Green.\\n\\nQuestion: What color is a banana peel?\\n\\nAnswer:  Brown.\\n\\nQuestion: What color is a banana skin?\\n\\nAnswer:  Brown.\\n\\nQuestion: What color is a banana shell?\\n\\nAnswer:  There is no such thing as a banana shell.\\n\\nQuestion: What color is a banana when it is not ripe?\\n\\nAnswer:  Green.\\n\\nQuestion: What color is a banana when it is overripe?\\n\\nAnswer:  Brown or black.\\n\\nQuestion: What color is a banana when it is slightly ripe?\\n\\nAnswer:  Yellow.\\n\\nQuestion: What color is a banana when it is very ripe?\\n\\nAnswer:  Dark yellow or brown.\\n\\nQuestion: What color is a banana when it is just starting to ripen?\\n\\nAnswer:  Green with yellow spots.\\n\\nQuestion: What color is a banana when it is fully ripe?\\n\\nAnswer:  Yellow.\\n\\nQuestion: What color is a banana when it is almost ripe?\\n\\nAnswer:  Yellow with brown spots.\\n\\nQuestion: What color is a banana when it is not ripe yet but starting to turn yellow?\\n\\nAnswer:  Green with yellow spots.\\n\\nQuestion: What color is a banana when it is ripe and ready to eat?\\n\\nAnswer:  Yellow.\\n\\nQuestion: What color is a banana when it is very ripe and mushy?\\n\\nAnswer:  Brown or black.\\n\\nQuestion: What color is a banana when it is slightly overripe?\\n\\nAnswer:  Dark yellow or brown.\\n\\nQuestion: What color is a banana when it is very overripe?\\n\\nAnswer:  Brown or black.\\n\\nQuestion: What color is a banana when it is not ripe at all?\\n\\nAnswer:  Green.\\n\\nQuestion: What color is a banana when it is just starting to ripen?\\n\\nAnswer:  Green with yellow spots.\\n\\nQuestion: What color is a banana when it is ripe and ready to eat?\\n\\nAnswer: ')]] llm_output=None run=[RunInfo(run_id=UUID('a9ef47b1-761a-4566-b642-7fb923152142')), RunInfo(run_id=UUID('97dd9d50-744d-483c-8873-b40518df976a')), RunInfo(run_id=UUID('d677a938-164c-4111-a3af-5323af6c552c')), RunInfo(run_id=UUID('c7bcdad8-408b-41df-88f8-0f754260e2d5'))]\n"
     ]
    }
   ],
   "source": [
    "res = llm_chain_mistral.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain_mistral = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm_mistral\n",
    ")\n",
    "\n",
    "llm_chain_t5 = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm_t5\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\"What color is a ripe banana?\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----T5-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the capital of France. It is also the largest city in the world. It is also the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth.\n",
      "------Mistral-----\n",
      "Answer the following questions one at a time.\n",
      "\n",
      "Questions:\n",
      "What is the capital city of France?\n",
      "What is the largest mammal on Earth?\n",
      "Which gas is most abundant in Earth's atmosphere?\n",
      "What color is a ripe banana?\n",
      "\n",
      "\n",
      "Answers:\n",
      "1. Paris\n",
      "2. Blue whale\n",
      "3. Nitrogen\n",
      "4. Yellow\n"
     ]
    }
   ],
   "source": [
    "print(\"-----T5-----\")\n",
    "print(llm_chain_t5.run(qs_str))\n",
    "print(\"------Mistral-----\")\n",
    "print(llm_chain_mistral.run(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain_t5 = LLMChain(llm=llm_t5, prompt=summarization_prompt)\n",
    "summarization_chain_mistral = LLMChain(llm=llm_mistral, prompt=summarization_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text_t5 = summarization_chain_t5.predict(text=text)\n",
    "summarized_text_mistral = summarization_chain_mistral.predict(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----T5-----\n",
      "Build an application.\n",
      "------Mistral-----\n",
      "Summarize the following text to one sentence: LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\n",
      "\n",
      "LangChain offers modules for building language model applications, allowing users to create complex or simple applications by combining or using individual modules, with the fundamental building block being the calling of a language model on input data, such as a service generating a company name based on the product it produces.\n"
     ]
    }
   ],
   "source": [
    "print(\"-----T5-----\")\n",
    "print(summarized_text_t5)\n",
    "print(\"------Mistral-----\")\n",
    "print(summarized_text_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain_t5 = LLMChain(llm=llm_t5, prompt=translation_prompt)\n",
    "translation_chain_mistral = LLMChain(llm=llm_mistral, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "text = \"My name is Bibek\"\n",
    "translated_text_t5 = translation_chain_t5.predict(source_language=source_language, target_language=target_language, text=text)\n",
    "translated_text_mistral = translation_chain_mistral.predict(source_language=source_language, target_language=target_language, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----T5------\n",
      "M. Bibek\n",
      "-----Mistral------\n",
      "Translate the following text from English to French: My name is Bibek and I am from Nepal. I am studying in France. I have been in France for one year. I am studying computer science. I like France and the French people.\n",
      "\n",
      "Mon nom est Bibek et je suis népalais. Je suis en France pour étudier. J'ai été en France pendant une année. Je suis en train d'étudier les sciences informatiques. Je aime la France et les Français.\n"
     ]
    }
   ],
   "source": [
    "print(\"-----T5------\")\n",
    "print(translated_text_t5)\n",
    "print(\"-----Mistral------\")\n",
    "print(translated_text_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
